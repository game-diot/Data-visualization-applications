import asyncio
import pandas as pd
from typing import Optional, Dict, Any

from src.shared.utils.logger import logger
from src.shared.exceptions.base import BaseAppException

# Schemas
from src.features.quality.schemas.quality_analysis import (
    QualityCheckResponse,
    MissingStatistics,
    DuplicateStatistics,
    AnomalyStatistics
)

# Repositories (æ•°æ®å±‚)
from src.features.quality.repository.dataset_repository import dataset_repository
from src.features.quality.repository.cache_repository import CacheRepository
from src.features.quality.repository.task_repository import TaskRepository

# Utils (è®¡ç®—å±‚)
from src.features.quality.utils import metrics, scoring
from src.features.quality.utils.validation import validate_file_for_analysis

class AnalysisService:
    """
    æ•°æ®è´¨é‡æ·±åº¦åˆ†ææœåŠ¡ (Analysis)
    
    åœºæ™¯ï¼šç”¨æˆ·ç‚¹å‡» 'å¼€å§‹æ£€æµ‹'ã€‚
    ç‰¹ç‚¹ï¼šè®¡ç®—å¯†é›†å‹ (CPU Bound)ï¼Œè€—æ—¶è¾ƒé•¿ï¼Œéœ€è¦æ›´æ–°ä»»åŠ¡è¿›åº¦ã€‚
    """

    def __init__(self):
        self.cache_repo = CacheRepository()
        self.task_repo = TaskRepository()

    async def perform_analysis(self, file_id: str, file_path: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡Œå…¨é‡æ•°æ®è´¨é‡åˆ†æ
        
        Args:
            file_id: æ–‡ä»¶å”¯ä¸€æ ‡è¯†
            file_path: æ–‡ä»¶ç»å¯¹è·¯å¾„ (ç”± Controller è§£æåä¼ å…¥)
            force_refresh: æ˜¯å¦å¼ºåˆ¶é‡ç®—
        """
        logger.info(f"ğŸš€ [Analysis] Request received for {file_id}")

        # 1. æ£€æŸ¥ç¼“å­˜ (å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°)
        if not force_refresh:
            cached_result = await self.cache_repo.get_analysis_result(file_id)
            if cached_result:
                logger.info(f"ğŸ¯ [Analysis] Cache hit for {file_id}")
                # ç¡®ä¿ä»»åŠ¡çŠ¶æ€ä¹Ÿæ˜¯å®Œæˆçš„ï¼Œé˜²æ­¢å‰ç«¯ä¸€ç›´ loading
                await self.task_repo.mark_completed(file_id, result_id=file_id)
                return cached_result

        # 2. åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€ (Pending)
        await self.task_repo.init_task(file_id)

        try:
            # 3. å¼€å§‹å¼‚æ­¥è®¡ç®—æµç¨‹
            # æ³¨æ„ï¼šPandas æ˜¯ CPU å¯†é›†å‹ï¼Œåº”è¯¥æ”¾åœ¨ ThreadPool ä¸­è¿è¡Œï¼Œä»¥å…é˜»å¡ FastAPI çš„ EventLoop
            # è¿™é‡Œæˆ‘ä»¬å°†æ ¸å¿ƒè®¡ç®—é€»è¾‘å°è£…åœ¨ _run_cpu_bound_analysis ä¸­
            result = await asyncio.to_thread(
                self._run_cpu_bound_analysis, 
                file_id, 
                file_path
            )

            # 4. å­˜å…¥ç¼“å­˜
            await self.cache_repo.save_analysis_result(file_id, result.model_dump())
            
            # 5. æ ‡è®°ä»»åŠ¡å®Œæˆ
            await self.task_repo.mark_completed(file_id, result_id=file_id)
            
            return result.model_dump()

        except Exception as e:
            logger.error(f"ğŸ’¥ [Analysis] Failed: {str(e)}", exc_info=True)
            # æ ‡è®°ä»»åŠ¡å¤±è´¥ï¼Œè®©å‰ç«¯çŸ¥é“
            await self.task_repo.mark_failed(file_id, error_msg=str(e))
            raise e

    def _run_cpu_bound_analysis(self, file_id: str, file_path: str) -> QualityCheckResponse:
        """
        [Sync] CPU å¯†é›†å‹è®¡ç®—é€»è¾‘
        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œå¯ä»¥å®‰å…¨åœ°ä½¿ç”¨é˜»å¡çš„ Pandas æ“ä½œ
        """
        
        # --- é˜¶æ®µ 1: åŠ è½½ (10%) ---
        validate_file_for_analysis(file_path)
        df = dataset_repository.load_dataframe(file_path, file_id)
        
        # æ—¢ç„¶åœ¨çº¿ç¨‹é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ run_coroutine_threadsafe æ›´æ–° Redisï¼Œ
        # ä½†ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œé€šå¸¸ä¸å»ºè®®åœ¨åŒæ­¥çº¿ç¨‹é‡Œåå‘è°ƒç”¨å¼‚æ­¥ Redisã€‚
        # å®é™…ç”Ÿäº§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ Celeryã€‚è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œå‡è®¾ä¸­é—´æ­¥éª¤ä¸æ›´æ–° Redisï¼Œ
        # æˆ–è€…åªåœ¨è¿™ä¸€å±‚åšè®¡ç®—ï¼ŒçŠ¶æ€æ›´æ–°ç”±å¤–å±‚æ§åˆ¶ï¼ˆç¨å¾®ç‰ºç‰²ä¸€ç‚¹ä¸­é—´è¿›åº¦æ¡çš„å®æ—¶æ€§ï¼‰ã€‚
        
        row_count = len(df)
        col_count = len(df.columns)

        # --- é˜¶æ®µ 2: åŸºç¡€æŒ‡æ ‡è®¡ç®— (ç¼ºå¤± & é‡å¤) ---
        # è°ƒç”¨ metrics æ¨¡å—
        missing_data = metrics.calculate_missing_stats(df)
        duplicate_data = metrics.calculate_duplicate_stats(df)
        
        # --- é˜¶æ®µ 3: æ·±åº¦æŒ‡æ ‡è®¡ç®— (å¼‚å¸¸å€¼) ---
        # è¿™ä¸€æ­¥æœ€è€—æ—¶
        anomaly_data = metrics.calculate_anomaly_stats(df, method='iqr')
        
        # --- é˜¶æ®µ 4: è¯„åˆ† & ç»„è£… ---
        types_map = metrics.infer_column_types(df)
        
        score = scoring.calculate_quality_score(
            missing_rate=missing_data['missing_rate'],
            duplicate_rate=duplicate_data['duplicate_rate'],
            # ä½¿ç”¨å¼‚å¸¸å€¼å æ€»è¡Œæ•°çš„æ¯”ä¾‹ä½œä¸ºæƒ©ç½šå› å­
            anomaly_rate=anomaly_data['total'] / (row_count * col_count) if row_count > 0 else 0
        )

        # æ„å»º Pydantic æ¨¡å‹ (è¿™ä¹Ÿèµ·åˆ°äº†æœ€åçš„æ ¡éªŒä½œç”¨)
        response = QualityCheckResponse(
            file_id=file_id,
            row_count=row_count,
            column_count=col_count,
            quality_score=score,
            missing=MissingStatistics(**missing_data),
            duplicates=DuplicateStatistics(**duplicate_data),
            anomalies=AnomalyStatistics(**anomaly_data),
            types=types_map
        )
        
        return response

    async def get_progress(self, file_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æä»»åŠ¡è¿›åº¦"""
        return await self.task_repo.get_task(file_id)

# å¯¼å‡ºå•ä¾‹
analysis_service = AnalysisService()