import asyncio
import pandas as pd
from typing import Optional, Dict, Any

from src.shared.utils.logger import logger
from src.shared.exceptions.base import BaseAppException
from src.shared.utils.json_helper import sanitize_json_values
# Schemas
from src.features.quality.schemas.quality_analysis import (
    QualityCheckResponse,
    MissingStatistics,
    DuplicateStatistics,
    AnomalyStatistics
)

# Repositories (æ•°æ®å±‚)
from src.features.quality.repository.dataset_repository import dataset_repository
from src.features.quality.repository.cache_repository import CacheRepository
from src.features.quality.repository.task_repository import TaskRepository

# Utils (è®¡ç®—å±‚)
from src.features.quality.utils import metrics, scoring
from src.features.quality.utils.validation import validate_file_for_analysis

class AnalysisService:
    """
    æ•°æ®è´¨é‡æ·±åº¦åˆ†ææœåŠ¡ (Analysis)
    
    åœºæ™¯ï¼šç”¨æˆ·ç‚¹å‡» 'å¼€å§‹æ£€æµ‹'ã€‚
    ç‰¹ç‚¹ï¼šè®¡ç®—å¯†é›†å‹ (CPU Bound)ï¼Œè€—æ—¶è¾ƒé•¿ï¼Œéœ€è¦æ›´æ–°ä»»åŠ¡è¿›åº¦ã€‚
    """

    def __init__(self):
        self.cache_repo = CacheRepository()
        self.task_repo = TaskRepository()

    async def perform_analysis(self, file_id: str, file_path: str, force_refresh: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œå…¨é‡æ•°æ®è´¨é‡åˆ†æ
        """
        logger.info(f"ğŸš€ [Analysis] Request received for {file_id}")

        # 1. æ£€æŸ¥ç¼“å­˜
        if not force_refresh:
            cached_result = await self.cache_repo.get_analysis_result(file_id)
            if cached_result:
                logger.info(f"ğŸ¯ [Analysis] Cache hit for {file_id}")
                await self.task_repo.mark_completed(file_id, result_id=file_id)
                return cached_result

        # 2. åˆå§‹åŒ–ä»»åŠ¡
        await self.task_repo.init_task(file_id)

        try:
            # 3. å¼‚æ­¥è®¡ç®—
            result = await asyncio.to_thread(
                self._run_cpu_bound_analysis, 
                file_id, 
                file_path
            )

            # 4. åºåˆ—åŒ–ä¸æ¸…æ´— (å…³é”®æ­¥éª¤!)
            # å…ˆè½¬æˆ Dict
            raw_dict = result.model_dump()
            
            # ğŸ”¥ æ¸…æ´— NaN/Infinity -> None
            clean_dict = sanitize_json_values(raw_dict)

            # 5. å­˜å…¥ç¼“å­˜ (å­˜æ¸…æ´—åçš„æ•°æ®)
            await self.cache_repo.save_analysis_result(file_id, clean_dict)
            
            # 6. æ ‡è®°å®Œæˆ
            await self.task_repo.mark_completed(file_id, result_id=file_id)
            
            # 7. è¿”å›ç»™ Controller
            return clean_dict

        except Exception as e:
            logger.error(f"ğŸ’¥ [Analysis] Failed: {str(e)}", exc_info=True)
            await self.task_repo.mark_failed(file_id, error_msg=str(e))
            raise e

    def _run_cpu_bound_analysis(self, file_id: str, file_path: str) -> QualityCheckResponse:
        """
        [Sync] CPU å¯†é›†å‹è®¡ç®—é€»è¾‘
        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œå¯ä»¥å®‰å…¨åœ°ä½¿ç”¨é˜»å¡çš„ Pandas æ“ä½œ
        """

        # --- é˜¶æ®µ 1: åŠ è½½ (10%) ---
        validate_file_for_analysis(file_path)
        df = dataset_repository.load_dataframe(file_path, file_id)
        
        # æ—¢ç„¶åœ¨çº¿ç¨‹é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ run_coroutine_threadsafe æ›´æ–° Redisï¼Œ
        # ä½†ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œé€šå¸¸ä¸å»ºè®®åœ¨åŒæ­¥çº¿ç¨‹é‡Œåå‘è°ƒç”¨å¼‚æ­¥ Redisã€‚
        # å®é™…ç”Ÿäº§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ Celeryã€‚è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œå‡è®¾ä¸­é—´æ­¥éª¤ä¸æ›´æ–° Redisï¼Œ
        # æˆ–è€…åªåœ¨è¿™ä¸€å±‚åšè®¡ç®—ï¼ŒçŠ¶æ€æ›´æ–°ç”±å¤–å±‚æ§åˆ¶ï¼ˆç¨å¾®ç‰ºç‰²ä¸€ç‚¹ä¸­é—´è¿›åº¦æ¡çš„å®æ—¶æ€§ï¼‰ã€‚

        row_count = len(df)
        col_count = len(df.columns)
  
        # --- é˜¶æ®µ 2: åŸºç¡€æŒ‡æ ‡è®¡ç®— (ç¼ºå¤± & é‡å¤) ---
        # è°ƒç”¨ metrics æ¨¡å—
        missing_data = metrics.calculate_missing_stats(df)
        duplicate_data = metrics.calculate_duplicate_stats(df)
    
        # --- é˜¶æ®µ 3: æ·±åº¦æŒ‡æ ‡è®¡ç®— (å¼‚å¸¸å€¼) ---
        # è¿™ä¸€æ­¥æœ€è€—æ—¶
        anomaly_data = metrics.calculate_anomaly_stats(df, method='iqr')
   
        # --- é˜¶æ®µ 4: è¯„åˆ† & ç»„è£… ---
        types_map = metrics.infer_column_types(df)
   
        score = scoring.calculate_quality_score(
            missing_rate=missing_data['missing_rate'],
            duplicate_rate=duplicate_data['duplicate_rate'],
            # ä½¿ç”¨å¼‚å¸¸å€¼å æ€»è¡Œæ•°çš„æ¯”ä¾‹ä½œä¸ºæƒ©ç½šå› å­
            anomaly_rate=anomaly_data['total'] / (row_count * col_count) if row_count > 0 else 0
        )
     
        # æ„å»º Pydantic æ¨¡å‹ (è¿™ä¹Ÿèµ·åˆ°äº†æœ€åçš„æ ¡éªŒä½œç”¨)
        response = QualityCheckResponse(
            file_id=file_id,
            row_count=row_count,
            column_count=col_count,
            quality_score=score,
            missing=MissingStatistics(**missing_data),
            duplicates=DuplicateStatistics(**duplicate_data),
            anomalies=AnomalyStatistics(**anomaly_data),
            types=types_map
        )
     
        return response

    async def get_progress(self, file_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æä»»åŠ¡è¿›åº¦"""
        return await self.task_repo.get_task(file_id) # type: ignore

# å¯¼å‡ºå•ä¾‹
analysis_service = AnalysisService()