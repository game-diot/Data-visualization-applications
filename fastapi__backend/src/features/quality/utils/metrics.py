# æ–‡ä»¶è·¯å¾„: src/features/quality/utils/metrics.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List

# =========================================================
# 1. ç¼ºå¤±å€¼åˆ†æ (Missing)
# =========================================================

def calculate_missing_stats(df: pd.DataFrame) -> Dict[str, Any]:
    """è®¡ç®—ç¼ºå¤±å€¼ç»Ÿè®¡"""
    total_cells = df.size
    total_missing = int(df.isnull().sum().sum())
    
    overall_rate = (total_missing / total_cells) if total_cells > 0 else 0.0
    
    missing_series = df.isnull().sum() / len(df)
    # è¿‡æ»¤æ‰ç¼ºå¤±ç‡ä¸º0çš„åˆ—ï¼Œåªè¿”å›æœ‰é—®é¢˜çš„åˆ—
    by_column = missing_series[missing_series > 0].round(4).to_dict()
    
    return {
        "total_missing_cells": total_missing,
        "missing_rate": round(overall_rate, 4),
        "by_column": by_column,
        "columns_with_missing": list(by_column.keys())
    }

# =========================================================
# 2. é‡å¤è¡Œåˆ†æ (Duplicates)
# =========================================================

def calculate_duplicate_stats(df: pd.DataFrame) -> Dict[str, Any]:
    """è®¡ç®—é‡å¤è¡Œç»Ÿè®¡"""
    total_rows = len(df)
    if total_rows == 0:
        return {"total_duplicate_rows": 0, "unique_duplicate_groups": 0, "duplicate_rate": 0.0, "rows": []}

    # keep='first' æ ‡è®°é™¤ç¬¬ä¸€æ¬¡å‡ºç°å¤–çš„æ‰€æœ‰é‡å¤é¡¹
    dup_mask = df.duplicated(keep='first')
    total_duplicates = int(dup_mask.sum())
    
    # åªæœ‰å½“æœ‰é‡å¤æ—¶æ‰è®¡ç®—ç»„æ•°ï¼ŒèŠ‚çœæ€§èƒ½
    unique_groups = 0
    if total_duplicates > 0:
        unique_groups = df[df.duplicated(keep=False)].drop_duplicates().shape[0]

    return {
        "total_duplicate_rows": total_duplicates,
        "unique_duplicate_groups": unique_groups,
        "duplicate_rate": round(total_duplicates / total_rows, 4),
        # è½¬æ¢ä¸º 1-based è¡Œå·ï¼Œæ–¹ä¾¿å‰ç«¯æ˜¾ç¤º
        "rows": (df[dup_mask].index + 1).tolist()
    }

# =========================================================
# 3. å¼‚å¸¸å€¼åˆ†æ (Anomalies - IQR & Z-score)
# =========================================================
def _is_likely_categorical(series: pd.Series, threshold_ratio: float = 0.05, threshold_count: int = 20) -> bool:
    """
    [Internal] åˆ¤æ–­ä¸€åˆ—æ•°å­—æ˜¯å¦åƒåˆ†ç±»å˜é‡ (Categorical/Ordinal)
    
    é€»è¾‘ï¼š
    1. å¦‚æœå”¯ä¸€å€¼æ•°é‡ (nunique) éå¸¸å°‘ (< 20)ï¼Œé€šå¸¸æ˜¯æšä¸¾ (å¦‚æ€§åˆ« 0/1ï¼Œæœˆä»½ 1-12ï¼Œè¯„åˆ† 1-5)ã€‚
    2. å¦‚æœå”¯ä¸€å€¼å æ¯” (nunique/count) éå¸¸ä½ (< 5%)ï¼Œè¯´æ˜å¤§é‡é‡å¤ï¼Œä¸é€‚åˆåšç¦»ç¾¤ç‚¹æ£€æµ‹ã€‚
    """
    # ç§»é™¤ç©ºå€¼åè®¡ç®—
    clean_series = series.dropna()
    if len(clean_series) == 0:
        return False
        
    n_unique = clean_series.nunique()
    ratio = n_unique / len(clean_series)
    
    # åˆ¤å®šæ¡ä»¶ï¼šå”¯ä¸€å€¼å¾ˆå°‘ OR å”¯ä¸€å€¼å æ¯”æä½
    # ä¾‹å¦‚ï¼š1000è¡Œæ•°æ®ï¼Œåªæœ‰ 10 ä¸ªä¸åŒçš„å€¼ -> True (è·³è¿‡æ£€æµ‹)
    if n_unique <= threshold_count:
        return True
    
    # å®½æ¾æ¨¡å¼ï¼šå¦‚æœä½ å¸Œæœ›æ›´ä¸¥æ ¼ï¼Œå¯ä»¥æŠŠè¿™ä¸ªæ¡ä»¶å»æ‰ï¼Œæˆ–è€… ratio è®¾å¾—æ›´å°
    # if ratio < threshold_ratio:
    #     return True
        
    return False

def _detect_iqr(series: pd.Series, col_name: str, multiplier: float = 3.0) -> List[Dict[str, Any]]:
    """
    [Internal] è®¡ç®—å•åˆ— IQR å¼‚å¸¸å€¼ (Extreme Outliers)
    
    Args:
        multiplier: é»˜è®¤ 3.0 (æç«¯å¼‚å¸¸å€¼)ï¼Œä¹‹å‰æ˜¯ 1.5 (å¸¸è§„å¼‚å¸¸å€¼)
    """
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    
    # é˜²å¾¡ï¼šå¦‚æœæ•°æ®æåº¦é›†ä¸­ (å¦‚ 75% çš„æ•°éƒ½æ˜¯åŒä¸€ä¸ª)ï¼ŒIQR ä¸º 0ï¼Œä¼šå¯¼è‡´è¯¯åˆ¤
    if IQR == 0:
        return []

    lower = Q1 - multiplier * IQR
    upper = Q3 + multiplier * IQR
    
    mask = (series < lower) | (series > upper)
    outliers = series[mask]
    
    details = []
    # é™åˆ¶è¿”å›æ•°é‡ï¼Œåªå–å‰ 50 ä¸ªå…¸å‹çš„ï¼Œé¿å…å‰ç«¯æ¸²æŸ“å¡æ­»
    # æ’åºï¼šå–åç¦»æœ€è¿œçš„ (æœ€å¤§æˆ–æœ€å°)
    top_outliers = outliers.sort_values(key=lambda x: abs(x - series.median()), ascending=False).head(50)

    for idx, val in top_outliers.items():
        details.append({
            # ğŸ”§ FIX: ç±»å‹è½¬æ¢ï¼Œç¡®ä¿ JSON åºåˆ—åŒ–å®‰å…¨
            "row": int(idx) + 1, # type: ignore
            "column": col_name,
            "value": float(val), 
            "type": "outlier_iqr",
            "reason": f"è¶…å‡ºæå€¼èŒƒå›´ [{lower:.2f}, {upper:.2f}] (IQR x {multiplier})"
        })
    return details

def calculate_anomaly_stats(df: pd.DataFrame, method: str = 'iqr') -> Dict[str, Any]:
    """
    è®¡ç®—å¼‚å¸¸å€¼ç»Ÿè®¡ (æ™ºèƒ½ä¼˜åŒ–ç‰ˆ)
    """
    # 1. åªé€‰æ•°å€¼åˆ—
    numeric_df = df.select_dtypes(include=np.number)
    
    all_details = []
    by_type = {"outlier_iqr": 0, "outlier_zscore": 0}
    by_column = {}
    
    for col in numeric_df.columns:
        series = numeric_df[col]
        
        # 2. â­ï¸ æ™ºèƒ½è·³è¿‡é€»è¾‘ï¼šå¦‚æœæ˜¯ ID åˆ—ã€æšä¸¾åˆ—ã€æœˆä»½åˆ—ç­‰ï¼Œè·³è¿‡æ£€æµ‹
        if _is_likely_categorical(series):
            # å¯ä»¥åœ¨æ—¥å¿—é‡Œè®°å½•ä¸€ä¸‹ï¼šlogger.debug(f"Skipping anomaly detection for categorical-like column: {col}")
            continue

        # 3. è®¡ç®— IQR (ä½¿ç”¨ 3.0 å€ç‡)
        column_anomalies = _detect_iqr(series, col, multiplier=3.0)
        
        if column_anomalies:
            count = len(column_anomalies)
            by_column[col] = count
            by_type["outlier_iqr"] += count
            all_details.extend(column_anomalies)

    return {
        "total": len(all_details),
        "by_type": by_type,
        "by_column": by_column,
        # æŒ‰è¡Œå·æ’åºï¼Œæ–¹ä¾¿å‰ç«¯å±•ç¤º
        "details": sorted(all_details, key=lambda x: x['row'])
    }
# =========================================================
# 4. ç±»å‹æ¨æ–­ (Type Inference)
# =========================================================

def infer_column_types(df: pd.DataFrame) -> Dict[str, str]:
    """
    æ¨æ–­æ¯åˆ—çš„æ•°æ®ç±»å‹ï¼Œç”¨äºè¿”å›ç»™å‰ç«¯å±•ç¤º
    
    Args:
        df: Pandas DataFrame
        
    Returns:
        Dict[str, str]: e.g. {"age": "int64", "name": "object", "score": "float64"}
    """
    # dtypes è¿”å›çš„æ˜¯ Seriesï¼Œç´¢å¼•æ˜¯åˆ—åï¼Œå€¼æ˜¯ dtype å¯¹è±¡
    # æˆ‘ä»¬ä½¿ç”¨ apply(str) å°† dtype å¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²
    return df.dtypes.apply(lambda x: str(x)).to_dict()