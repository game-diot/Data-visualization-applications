import pandas as pd
import logging
import asyncio
from src.modules.quality.repository.file_repository import FileRepository
from src.modules.quality.repository.cache_repository import CacheRepository
from src.modules.quality.utils.data_summary import (
    calculate_missing_rate, detect_duplicates,
    analyze_types, calculate_quality_score
)
from src.modules.quality.utils.preview_builder import build_preview, sample_data
from src.modules.quality.utils.cache_key_generator import get_cache_key
from fastapi__backend.src.shared.exceptions.tepe import FileNotFoundException,DataParseException,ValidationException


logger = logging.getLogger(__name__)

class QualityService:
    def __init__(self):
        self.file_repo = FileRepository()
        self.cache_repo = CacheRepository()

    async def analyze(self, file_path: str, sample_rows: int = 50, force_refresh: bool = False):
        """
        ä¸»åˆ†ææµç¨‹ï¼š
        1. æ£€æŸ¥ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
        2. è¯»å–æ–‡ä»¶
        3. è®¡ç®—ç»Ÿè®¡ä¸è´¨é‡æŒ‡æ ‡
        4. ç”Ÿæˆé¢„è§ˆ
        5. å†™å…¥ç¼“å­˜
        """
        cache_key = get_cache_key(file_path, {"sample_rows": sample_rows})
        logger.info(f"ğŸ” Starting analysis for file: {file_path}")

        # Step 1: ç¼“å­˜æ£€æŸ¥
        if not force_refresh:
            cached_result = await self.cache_repo.get_quality_result(cache_key)
            if cached_result:
                logger.info("âœ… Cache hit - returning cached result")
                return cached_result

        # Step 2: æ–‡ä»¶è¯»å–

        if file_path.endswith(".csv"):
            df = await asyncio.to_thread(self.file_repo.read_csv, file_path)
        else:
            df = await asyncio.to_thread(self.file_repo.read_excel, file_path)

        if df is None or df.empty:
            raise DataParseException("æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")

        # Step 3: æ•°æ®è´¨é‡åˆ†æ
        missing_rate = calculate_missing_rate(df)
        duplicates = detect_duplicates(df)
        types = analyze_types(df)
        summary = {
            "rows": len(df),
            "columns": len(df.columns),
            "missing_rate": missing_rate,
            "duplicates": duplicates,
        }

        quality_score = calculate_quality_score(summary)

        # Step 4: æ•°æ®é¢„è§ˆ
        preview = sample_data(df, sample_rows)

        result = {
            "preview": preview,
            "summary": {**summary, "quality_score": quality_score},
            "types": types,
        }

        # Step 5: å†™å…¥ç¼“å­˜
        await self.cache_repo.set_quality_result(cache_key, result, ttl=3600)
        logger.info("âœ… Analysis complete and cached")

        return result

    async def get_preview(self, file_path: str, limit: int = 10):
        """
        è·å–æ–‡ä»¶é¢„è§ˆï¼š
        ä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼Œæœªå‘½ä¸­åˆ™é‡æ–°åˆ†æã€‚
        """
        cache_key = get_cache_key(file_path, {"preview_limit": limit})
        cached = await self.cache_repo.get_quality_result(cache_key)
        if cached:
            logger.info("âœ… Preview cache hit")
            return cached.get("preview")

        logger.info("âš™ï¸  Cache miss - analyzing file for preview")
        result = await self.analyze(file_path=file_path, sample_rows=limit, force_refresh=False)
        return result.get("preview")

    async def clear_cache(self, file_path: str):
        """
        æ¸…é™¤æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜ã€‚
        """
        await self.cache_repo.delete_quality_result(file_path)
        logger.info(f"ğŸ§¹ Cache cleared for file: {file_path}")
        return {"message": "Cache cleared successfully"}

    async def get_task_status(self, task_id: str):
        """
        æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        """
        # å¦‚æœåç»­æ·»åŠ å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ï¼Œå¯ä»¥é€šè¿‡ TaskRepository æŸ¥è¯¢çŠ¶æ€
        return {"task_id": task_id, "status": "completed", "progress": 100.0}
