# ä¼˜åŒ–åçš„ quality_service.py

import pandas as pd
import asyncio
# å¯¼å…¥æˆ‘ä»¬ç»Ÿä¸€çš„æ—¥å¿—
from src.app.config.logging import app_logger as logger 
# å¯¼å…¥ I/O æŠ½è±¡å±‚
from src.modules.quality.repository.file_repository import FileRepository
# å¯¼å…¥å…±äº«ç¼“å­˜ç®¡ç†å™¨
from src.cache.cache_manager import CacheManager 

# å¯¼å…¥åˆ†æå·¥å…·ï¼ˆä¿æŒè·¯å¾„ä¸å˜ï¼Œå‡è®¾å·²å­˜åœ¨ï¼‰
from src.modules.quality.utils.data_summary import (
    calculate_missing_rate, detect_duplicates,
    analyze_types, calculate_quality_score
)
from src.modules.quality.utils.preview_builder import sample_data 
# å¯¼å…¥å…±äº«å·¥å…·
from src.shared.utils.cache_key_generator import get_cache_key 

# å¯¼å…¥è‡ªå®šä¹‰å¼‚å¸¸ï¼ˆå‡è®¾è·¯å¾„å·²ä¿®å¤ï¼‰
from src.shared.exceptions.type import DataParseException 
# æç¤ºï¼šFileNotFoundExceptionåº”åœ¨FileRepositoryä¸­å¤„ç†

# ä½¿ç”¨ CacheManager æ›¿ä»£ CacheRepository
class QualityService:
    def __init__(self):
        # è´Ÿè´£æ–‡ä»¶è¯»å–çš„ Repository
        self.file_repo = FileRepository() 
        # ç›´æ¥ä½¿ç”¨å…±äº« CacheManager å®ä¾‹
        self.cache_manager = CacheManager()
        # å®šä¹‰ç¼“å­˜ TTL (ä¾‹å¦‚ 7 å¤©)
        self.TTL_SECONDS = 7 * 24 * 3600

    async def _read_file_to_df(self, file_path: str) -> pd.DataFrame:
        """ è¾…åŠ©æ–¹æ³•ï¼šå°†æ–‡ä»¶è¯»å–æŠ½è±¡åˆ° FileRepository å¹¶è½¬æ¢ä¸ºå¼‚æ­¥ """
        try:
            # å‡è®¾ FileRepository ä¸­æœ‰ä¸€ä¸ªé€šç”¨çš„å¼‚æ­¥è¯»å–æ–¹æ³•
            df = await asyncio.to_thread(self.file_repo.read_file, file_path)  # type: ignore
            # åŸå§‹é€»è¾‘ï¼š
            # if file_path.endswith(".csv"):
            #     df = await asyncio.to_thread(self.file_repo.read_csv, file_path)
            # else:
            #     df = await asyncio.to_thread(self.file_repo.read_excel, file_path)
        except Exception as e:
            logger.error(f"File read error: {e}")
            # è¿™é‡Œåº”æ ¹æ®å…·ä½“çš„ FileRepository å¼‚å¸¸æ¥æ•è·
            raise DataParseException(f"æ–‡ä»¶è¯»å–æˆ–æ ¼å¼è§£æå¤±è´¥: {e}")

        if df is None or df.empty:
            raise DataParseException("æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
        return df

    async def analyze(self, file_path: str, sample_rows: int = 50, force_refresh: bool = False):
        """
        ä¸»åˆ†ææµç¨‹ï¼šå¢åŠ å¼‚å¸¸å€¼åˆ†æï¼Œå¹¶ä½¿ç”¨å…±äº« CacheManagerã€‚
        """
        # 1. ç”Ÿæˆç¼“å­˜é”® (ç¼“å­˜åªä¾èµ–äºæ–‡ä»¶å†…å®¹å’ŒæŠ½æ ·è¡Œæ•°)
        cache_key = get_cache_key(file_path, {"sample_rows": sample_rows})
        logger.info(f"ğŸ” Starting analysis for file: {file_path}. Key: {cache_key}")

        # Step 1: ç¼“å­˜æ£€æŸ¥
        if not force_refresh:
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result:
                logger.info("âœ… Cache hit - returning cached result")
                return cached_result

        # Step 2: æ–‡ä»¶è¯»å–
        df = await self._read_file_to_df(file_path)

        # Step 3: æ•°æ®è´¨é‡åˆ†æï¼ˆä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œè€—æ—¶è®¡ç®—ï¼‰
        def _run_sync_analysis(df, sample_rows):
            # è´¨é‡æŒ‡æ ‡è®¡ç®—
            missing_rate = calculate_missing_rate(df)
            duplicates = detect_duplicates(df)
            types = analyze_types(df)
            
            summary = {
                "rows": len(df),
                "columns": len(df.columns),
                "missing_rate": missing_rate,
                "duplicates": duplicates,
            }

            quality_score = calculate_quality_score(summary)
            
            # æ•°æ®é¢„è§ˆï¼ˆéšæœºé‡‡æ ·ï¼Œè¿”å›å­—å…¸åˆ—è¡¨ï¼‰
            preview = sample_data(df, sample_rows)
            
            return {
                "preview": preview,
                "summary": {**summary, "quality_score": quality_score},
                "types": types,
            }

        # å°†æ‰€æœ‰åŒæ­¥çš„ Pandas/Numpy è®¡ç®—æ”¾åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­æ‰§è¡Œ
        result = await asyncio.to_thread(_run_sync_analysis, df, sample_rows)
        
        # Step 4: å†™å…¥ç¼“å­˜
        await self.cache_manager.set(cache_key, result, ttl=self.TTL_SECONDS)
        logger.info("âœ… Analysis complete and cached")

        return result

    async def clear_cache(self, file_path: str):
        """
        æ¸…é™¤ä¸æ–‡ä»¶å†…å®¹ç›¸å…³çš„æ‰€æœ‰ç¼“å­˜ã€‚
        æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ¸…é™¤æ‰€æœ‰ä¾èµ–è¯¥æ–‡ä»¶å“ˆå¸Œçš„é”®ã€‚
        """
        # ä¸ºäº†ä¿è¯æ¸…é™¤æ‰€æœ‰åŸºäºè¯¥æ–‡ä»¶ç”Ÿæˆçš„é”®ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€šé…ç¬¦æˆ–æ›´ç²¾ç¡®çš„æ¨¡å¼åŒ¹é…ã€‚
        # å‡è®¾ get_cache_key ç”Ÿæˆçš„é”®æ˜¯ 'quality:{file_hash}:...'
        # æ¸…é™¤ç­–ç•¥ï¼šå…ˆç”Ÿæˆæ–‡ä»¶å“ˆå¸Œï¼Œå†æ¸…é™¤ 'quality:{file_hash}:*' æ¨¡å¼
        from src.shared.utils.cache_key_generator import generate_file_hash 
        
        try:
             file_hash = generate_file_hash(file_path)
             pattern = f"quality:{file_hash}:*"
             deleted_count = await self.cache_manager.clear_pattern(pattern)
             logger.info(f"ğŸ§¹ Cache cleared for pattern {pattern}. Count: {deleted_count}")
             return {"message": f"Cache cleared successfully. Deleted {deleted_count} items."}
        except FileNotFoundError:
             logger.warning(f"File not found during cache clear check: {file_path}. Cannot generate hash pattern.")
             return {"message": "File not found, skipping cache clear."}
        except Exception as e:
             logger.error(f"Error during cache pattern clear: {e}")
             return {"message": "Error occurred during cache clear."}

    async def get_task_status(self, task_id: str):
        """
        æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼šåº”è¯¥ä½¿ç”¨ TaskRepository (æˆ‘ä»¬ä¹‹å‰ä¿®æ”¹çš„é‚£ä¸ª)ã€‚
        """
        # å¯¼å…¥æˆ‘ä»¬ä¹‹å‰ä¿®æ”¹çš„ TaskRepository
        from src.modules.quality.repository.task_repository import TaskRepository 
        
        status = await TaskRepository.get_task_status(task_id)
        if status is None:
            return {"task_id": task_id, "status": "NOT_FOUND"}
        return status